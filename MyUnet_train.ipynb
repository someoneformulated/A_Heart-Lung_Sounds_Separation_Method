{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceramic-boundary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:46:05.639363Z",
     "start_time": "2021-09-13T11:46:04.465239Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#import wfdb\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "surgical-foster",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:46:05.648291Z",
     "start_time": "2021-09-13T11:46:05.640354Z"
    }
   },
   "outputs": [],
   "source": [
    "class myDataset(Dataset):#dataset\n",
    "    def __init__(self,mix,m_h,sig_num,seq_len,input_size):\n",
    "        #path\n",
    "        #  x0:TF     x1:mask of HS    x2：mask of LS\n",
    "        x0=np.loadtxt(open(mix,\"rb\"),delimiter=\",\",skiprows=0)\n",
    "        tempt=np.zeros([sig_num,seq_len,input_size]) \n",
    "        for i in range(sig_num):  #\n",
    "            for hang in range(seq_len):\n",
    "                for lie in range(input_size):\n",
    "                    tempt[i,hang,lie]=x0[hang+i*seq_len,lie]\n",
    "        x0=tempt.astype('float32')\n",
    "        \n",
    "        x1=np.loadtxt(open(m_h,\"rb\"),delimiter=\",\",skiprows=0)\n",
    "        tempt=np.zeros([sig_num,seq_len,input_size]) \n",
    "        for i in range(sig_num):  #\n",
    "            for hang in range(seq_len):\n",
    "                for lie in range(input_size):\n",
    "                    tempt[i,hang,lie]=x1[hang+i*seq_len,lie]\n",
    "        x1=tempt.astype('float32')    \n",
    "        \n",
    "        x0=torch.from_numpy(x0);x1=torch.from_numpy(x1);\n",
    "\n",
    "        x0=Variable(x0,requires_grad=True);x1=Variable(x1,requires_grad=True);\n",
    "        \n",
    "        x0=x0[:,np.newaxis,:,:];x1=x1[:,np.newaxis,:,:];\n",
    "        x0.cuda(0);x1.cuda(0);\n",
    "\n",
    "        self.len = x0.shape[0] # shape\n",
    "        self.x0_data=x0;self.x1_data=x1;\n",
    "        \n",
    "   \n",
    "    def __getitem__(self, index):    \n",
    "        #getitem是一个magic方法，可以对将来的实例对象执行下标操作,可以通过索引index，将里面的数据拿出来\n",
    "        return self.x0_data[index].cuda(0),self.x1_data[index].cuda(0)\n",
    "    \n",
    "    def __len__(self): #return the number of dataset\n",
    "        return self.len\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "structured-bandwidth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:46:56.694791Z",
     "start_time": "2021-09-13T11:46:05.651282Z"
    }
   },
   "outputs": [],
   "source": [
    "######################impeordata\n",
    "batch_train=1          #\n",
    "seq_len=256         #\n",
    "input_size=128      #\n",
    "\n",
    "\n",
    "# dataset=myDataset(mix=\"train0_mix鼾声_875.csv\",m_h=\"train0_m_h鼾声_875.csv\",\n",
    "#                   sig_num=875,seq_len=seq_len,input_size=input_size)\n",
    "# train_loader = DataLoader(dataset=dataset, batch_size=batch_train, shuffle=False,num_workers=0) \n",
    "\n",
    "\n",
    "dataset=myDataset(mix=\"mix_60.csv\",m_h=\"m_h_60.csv\",\n",
    "                  sig_num=20,seq_len=seq_len,input_size=input_size)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_train, shuffle=False,num_workers=0) \n",
    "\n",
    "\n",
    "dataset=myDataset(mix=\"mix_12.csv\",m_h=\"m_h_12.csv\",\n",
    "                  sig_num=12,seq_len=seq_len,input_size=input_size)\n",
    "test_loader = DataLoader(dataset=dataset, batch_size=4, shuffle=False,num_workers=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "standing-right",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:46:56.843356Z",
     "start_time": "2021-09-13T11:46:56.695719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "20\n",
      " x0：mix的尺寸:torch.Size([1, 1, 256, 128]),类型:torch.float32,梯度:True\n",
      " x1：m_h的尺寸:torch.Size([1, 1, 256, 128]),类型:torch.float32,梯度:True\n",
      "**************************************************\n",
      "3\n",
      " x0：mix的尺寸:torch.Size([4, 1, 256, 128]),类型:torch.float32,梯度:True\n",
      " x1：m_h的尺寸:torch.Size([4, 1, 256, 128]),类型:torch.float32,梯度:True\n",
      "torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "for index, (x0,x1,) in enumerate(train_loader,0): \n",
    "    pass\n",
    "print('*'*50);print(index+1)\n",
    "print(\" x0：mix的尺寸:{},类型:{},梯度:{}\".format(x0.shape,x0.dtype,x0.requires_grad))\n",
    "print(\" x1：m_h的尺寸:{},类型:{},梯度:{}\".format(x1.shape,x1.dtype,x1.requires_grad))\n",
    "\n",
    "for index, (x0,x1) in enumerate(test_loader,0): \n",
    "    pass\n",
    "print('*'*50);print(index+1)\n",
    "print(\" x0：mix的尺寸:{},类型:{},梯度:{}\".format(x0.shape,x0.dtype,x0.requires_grad))\n",
    "print(\" x1：m_h的尺寸:{},类型:{},梯度:{}\".format(x1.shape,x1.dtype,x1.requires_grad))\n",
    "##[batch,channel,H,W]\n",
    "\n",
    "print(x0.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "noted-colors",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:46:56.878326Z",
     "start_time": "2021-09-13T11:46:56.862727Z"
    }
   },
   "outputs": [],
   "source": [
    "#定义模型\n",
    "class MyUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #coding（3+1）\n",
    "        self.bn1= nn.BatchNorm2d(48);self.bn2= nn.BatchNorm2d(96);self.bn3= nn.BatchNorm2d(144);\n",
    "        self.bn0= nn.BatchNorm2d(1);\n",
    "        self.encode1_1 =nn.Conv2d(1,48,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.encode1_2=nn.Conv2d(48,48,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.encode2_1=nn.Conv2d(48,96,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.encode2_2=nn.Conv2d(96,96,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.encode3_1=nn.Conv2d(96,144,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.encode3_2=nn.Conv2d(144,144,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.encode4_1=nn.Conv2d(144,144,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.encode4_2=nn.Conv2d(144,144,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "        #decoding（3+1个组）\n",
    "        self.upsample3= nn.ConvTranspose2d(144,144,kernel_size=2,stride=2,padding=0);                                       \n",
    "        self.decode3_1 =nn.Conv2d(288,144,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.decode3_2 =nn.Conv2d(144,144,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "        self.upsample2= nn.ConvTranspose2d(144,144,kernel_size=2,stride=2,padding=0);\n",
    "        self.decode2_1 =nn.Conv2d(240,96,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.decode2_2 =nn.Conv2d(96,96,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "        self.upsample1= nn.ConvTranspose2d(96,96,kernel_size=2,stride=2,padding=0);\n",
    "        self.decode1_1 =nn.Conv2d(144,48,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.decode1_2 =nn.Conv2d(48,48,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "        self.decode0=nn.Conv2d(48,1,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        enc1 = torch.relu(self.encode1_1(x))\n",
    "        enc1 = torch.relu(self.encode1_2(enc1))\n",
    "        enc1 =self.bn1(enc1)\n",
    "        \n",
    "        enc2 = F.max_pool2d(enc1, kernel_size=2, stride=2, padding=0)\n",
    "        enc2 = torch.relu(self.encode2_1(enc2))\n",
    "        enc2 = torch.relu(self.encode2_2(enc2))\n",
    "        enc2 =self.bn2(enc2)\n",
    "        \n",
    "        enc3 = F.max_pool2d(enc2, kernel_size=2, stride=2, padding=0)\n",
    "        enc3 = torch.relu(self.encode3_1(enc3))\n",
    "        enc3 = torch.relu(self.encode3_2(enc3))\n",
    "        enc3 =self.bn3(enc3)\n",
    "        \n",
    "        enc4 = F.max_pool2d(enc3, kernel_size=2, stride=2, padding=0)       \n",
    "        enc4 = torch.relu(self.encode4_1(enc4))\n",
    "        enc4 = torch.relu(self.encode4_2(enc4))\n",
    "        enc4 =self.bn3(enc4)\n",
    "        \n",
    "        #解码部分\n",
    "        dec = self.upsample3(enc4)\n",
    "        dec = torch.relu(self.decode3_1(torch.cat([enc3,dec],dim=1)))\n",
    "        dec = torch.relu(self.decode3_2(dec))\n",
    "        dec =self.bn3(dec)\n",
    "        \n",
    "        dec = self.upsample2(dec)\n",
    "        dec = torch.relu(self.decode2_1(torch.cat([enc2,dec],dim=1)))\n",
    "        dec = torch.relu(self.decode2_2(dec))\n",
    "        dec =self.bn2(dec)\n",
    "    \n",
    "        dec = self.upsample1(dec)\n",
    "        dec = torch.relu(self.decode1_1(torch.cat([enc1,dec],dim=1)))\n",
    "        dec = torch.relu(self.decode1_2(dec))\n",
    "        dec =self.bn1(dec)\n",
    "        \n",
    "        out = self.bn0(torch.tanh(self.decode0(dec)))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ahead-hungarian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:46:56.887828Z",
     "start_time": "2021-09-13T11:46:56.878680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce 940MX'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.cuda.get_device_name(0)\n",
    "#torch.cuda.device_count()\n",
    "torch.cuda.is_available()\n",
    "\n",
    "print(torch.cuda.is_available()) # true GPU avaiable\n",
    "\n",
    "print(torch.cuda.device_count()) #GPU num， 1\n",
    "\n",
    "torch.cuda.current_device() # index of GPU， 0\n",
    "\n",
    "torch.cuda.get_device_name(0) #GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afraid-accommodation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:46:56.896506Z",
     "start_time": "2021-09-13T11:46:56.888830Z"
    }
   },
   "outputs": [],
   "source": [
    "#########定义损失函数\n",
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,out_pred,mix,m_h):\n",
    "        lossh = mix* (out_pred - m_h)           #[[B, C, H, W]]\n",
    "#         lossl = mix* ((1-out_pred) - m_l)           #[[B, C, H, W]]\n",
    "        loss=torch.sum(torch.sum(torch.pow(lossh, 2), 1))#\n",
    "        return loss\n",
    "    \n",
    "criterion = My_loss()\n",
    "# criterion = nn.MSELoss(reduction='sum')  # 用于计算MSE损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dense-handbook",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T11:46:56.923349Z",
     "start_time": "2021-09-13T11:46:56.897503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyUnet(\n",
       "  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (encode1_1): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (encode1_2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (encode2_1): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (encode2_2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (encode3_1): Conv2d(96, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (encode3_2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (encode4_1): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (encode4_2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (upsample3): ConvTranspose2d(144, 144, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decode3_1): Conv2d(288, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (decode3_2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (upsample2): ConvTranspose2d(144, 144, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decode2_1): Conv2d(240, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (decode2_2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (upsample1): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decode1_1): Conv2d(144, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (decode1_2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (decode0): Conv2d(48, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## hyper parameter\n",
    "\n",
    "lr = 0.01\n",
    "model = MyUnet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler  = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n",
    "optimizer.step()#\n",
    "scheduler.step()\n",
    "\n",
    "criterion.cuda(0)\n",
    "model.cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "uniform-arbitration",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-13T11:46:27.450Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "|epoch   0|time: 3.89s|train_loss:12452.30|\n",
      "**********************************************************************\n",
      "|epoch  0|time:4.71s|test loss:106997.30|\n",
      "**********************************************************************\n",
      "----------------------------------------------------------------------\n",
      "|epoch   1|time: 3.86s|train_loss:7135.78|\n",
      "----------------------------------------------------------------------\n",
      "|epoch   2|time: 3.88s|train_loss:6076.03|\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a877a874163d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\path\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\path\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_step=10#test model every 10 epchos\n",
    "for epoch in range(200):\n",
    "    start_time = time.time()\n",
    "    model.train()# training\n",
    "    mean_loss = 0.\n",
    "    for index, (mix,m_h) in enumerate(train_loader,0): \n",
    "#         mix=mix.cuda(0);m_h=m_h.cuda(0)\n",
    "        optimizer.zero_grad()\n",
    "        out_pred= model(mix)\n",
    "        loss = criterion(out_pred,mix,m_h)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mean_loss += loss.data.item()\n",
    "    mean_loss /= (index+1)\n",
    "    print('-' * 70)\n",
    "    print('|epoch{:4d}|time:{:5.2f}s|train_loss:{:5.2f}|'.format(epoch,(time.time()-start_time),mean_loss))\n",
    "    if epoch %log_step == 0:\n",
    "        torch.save(model.state_dict(),'MyUnet_0_epoch=%d.pt' % (epoch))\n",
    "        model.eval()\n",
    "        mean_loss=torch.tensor(0).float()\n",
    "        for index, (mix,m_h) in enumerate(test_loader,0):\n",
    "#             mix=mix.cuda(0);m_h=m_h.cuda(0)\n",
    "            out_pred=model(mix)\n",
    "            loss = criterion(out_pred,mix,m_h)\n",
    "            \n",
    "            mean_loss+= loss.data.item()\n",
    "        mean_loss /= (index+1)      \n",
    "        print('*' * 70)\n",
    "        print('|epoch{:3d}|time:{:4.2f}s|test loss:{:5.2f}|'.format(epoch,(time.time()-start_time),mean_loss))\n",
    "        print('*' * 70)     \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9565203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-stomach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-collector",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-13T11:46:30.202Z"
    }
   },
   "outputs": [],
   "source": [
    "#保存模型\n",
    "torch.save(model.state_dict(), 'my_unet0_鼾声.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-wrapping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-authentication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#输入验证信号：\n",
    "dataset=myDataset(mix=\"mix_20.csv\",m_h=\"m_h_20.csv\",\n",
    "                  sig_num=20,seq_len=seq_len,input_size=input_size)\n",
    "valid_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False,num_workers=0) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "for index, (mix,m_h) in enumerate(valid_loader,0):\n",
    "    if encuda:\n",
    "        mix=mix.cuda();m_h=m_h.cuda();\n",
    "    out_pred=model(mix)\n",
    "    loss = criterion(out_pred,mix,m_h)\n",
    "    print('第{:3d}个信号的loss: {:5.2f}'.format(index+1,loss))\n",
    "    # -----------------输出预测的mask-------------\n",
    "    out_h=out_pred;out_l=1-out_pred\n",
    "    h_mask=torch.reshape(out_h,(256,128));l_mask=torch.reshape(out_l,(256,128))\n",
    "\n",
    "    h_mask=h_mask.cpu();l_mask=l_mask.cpu()\n",
    "    h_mask=h_mask.detach().numpy();l_mask=l_mask.detach().numpy()\n",
    "    np.savetxt('h_mask%d.csv'%(index+1), h_mask, delimiter = ',')\n",
    "    np.savetxt('l_mask%d.csv'%(index+1), l_mask, delimiter = ',')\n",
    "#     print(h_mask.shape);print(l_mask.shape)\n",
    "    # -----------------输出预测的mask-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-drive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-congo",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
